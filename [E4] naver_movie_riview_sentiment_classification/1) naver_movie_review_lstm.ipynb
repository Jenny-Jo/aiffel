{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
    "$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
    "$ mv ratings_*.txt ~/aiffel/sentiment_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print('run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습때 다루었던 IMDB 데이터셋은 텍스트를 가공하여 imdb.data_loader() 메소드를 호출하면 숫자 인덱스로 변환된 텍스트와 word_to_index 딕셔너리까지 친절하게 제공합니다. 그러나 이번에 다루게 될 nsmc 데이터셋은 전혀 가공되지 않은 텍스트 파일로 이루어져 있습니다. 이것을 읽어서 imdb.data_loader()와 동일하게 동작하는 자신만의 data_loader를 만들어 보는 것으로 시작합니다. data_loader 안에서는 다음을 수행해야 합니다.\n",
    "\n",
    "* 데이터의 중복 제거\n",
    "* NaN 결측치 제거\n",
    "* 한국어 토크나이저로 토큰화\n",
    "* 불용어(Stopwords) 제거\n",
    "* 사전word_to_index 구성\n",
    "* 텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    "* X_train, y_train, X_test, y_test, word_to_index 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words = 10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how='any')\n",
    "    test_data.drop_duplicates(subset=['document'], inplace = True)\n",
    "    test_data = test_data.dropna(how='any')\n",
    "    \n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords]\n",
    "        X_train.append(temp_X)\n",
    "    \n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords]\n",
    "        X_test.append(temp_X)\n",
    "    \n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word : index for index, word in enumerate(vocab)}\n",
    "    \n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "    \n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "    \n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "    \n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 구성을 위한 데이터 분석 및 가공 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터셋 내 문장 길이 분포\n",
    "* 적절한 최대 문장 길이 지정\n",
    "* keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 116\n",
      "리뷰의 평균 길이 : 15.981687211831826\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 샘플 중 길이가 max_len 이하인 샘플의 비율이 몇 %인지 확인하는 함수\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 40 이하인 샘플의 비율: 93.0490758096072\n"
     ]
    }
   ],
   "source": [
    "max_len = 40\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 구성 및 validation set 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "모델은 3가지 이상 다양하게 구성하여 실험해 보세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 모델 훈련 개시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run GatherV2: Dst tensor is not initialized. [Op:GatherV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-bcdaae3d7521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# validation set은  fit 할 때 할당해주었다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    795\u001b[0m           data_adapter.train_validation_split((x, y, sample_weight),\n\u001b[1;32m    796\u001b[0m                                               \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                                               shuffle=False))\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   train_arrays = nest.map_structure(\n\u001b[0;32m-> 1338\u001b[0;31m       functools.partial(_split, indices=train_indices), arrays)\n\u001b[0m\u001b[1;32m   1339\u001b[0m   val_arrays = nest.map_structure(\n\u001b[1;32m   1340\u001b[0m       functools.partial(_split, indices=val_indices), arrays)\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_split\u001b[0;34m(t, indices)\u001b[0m\n\u001b[1;32m   1333\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   train_arrays = nest.map_structure(\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, validate_indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   4539\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4540\u001b[0m       \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4541\u001b[0;31m       batch_dims=batch_dims)\n\u001b[0m\u001b[1;32m   4542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4522\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4524\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   3753\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3754\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3755\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3756\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3757\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbatch_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run GatherV2: Dst tensor is not initialized. [Op:GatherV2]"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2) \n",
    "# validation set은  fit 할 때 할당해주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. loss, accuracy 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.plot(epochs, acc, 'r',label='acc')\n",
    "plt.title('Training and validation loss and acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss and acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 학습된 embeding layer 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자체학습한 임베딩 분석? 이걸 어떻게 해야할 지..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec.load('/home/aiffel/aiffel/naver_review/ko/ko.bin')\n",
    "\n",
    "print('사랑: ',word2vec_model.most_similar(positive=[\"사랑\"]))\n",
    "print('영웅: ',word2vec_model.most_similar(positive=[\"영웅\"]))\n",
    "print('파국: ',word2vec_model.most_similar(positive=[\"파국\"]))\n",
    "\n",
    "# 이미 학습한 레이어를 가지고 와서 사랑,영웅,파국을 분석하였다. 비슷 비슷한 단어를 가져온 걸로보아 성능은 괜찮은 것으로 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 한국어 word2vec 임베딩 활용하여 성능개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "\n",
    "vocab_size = 30185    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "def get_vector(word):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None\n",
    " \n",
    "for word, i in t.word_index.items(): # 훈련 데이터의 단어 집합에서 단어와 정수 인덱스를 1개씩 꺼내온다.\n",
    "    temp = get_vector(word) # 단어(key) 해당되는 임베딩 벡터의 300개의 값(value)를 임시 변수에 저장\n",
    "    if temp is not None: # 만약 None이 아니라면 임베딩 벡터의 값을 리턴받은 것이므로\n",
    "        embedding_matrix[i] = temp # 해당 단어 위치의 행에 벡터의 값을 저장한다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=max_len, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(LSTM(128))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고\n",
    "너무 어려워서 이해하는 데에 오래걸렸고, 팀원분들과 같이 하면서 차근차근 배워나가니 이해가 되었다. 여전히 모르는 게 많지만 조금씩 하다보면 잘 될거라 믿는다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
